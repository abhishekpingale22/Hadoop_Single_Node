------------RANGER Installation--------------------------------------------------------------------------------------
#Create database for ranger and ranger_audit in MYSQL.

Specify credentials : DB name, DB Username , DB Password

#Login to Ranger UI on Ranger-IP:6080

Ranger > Config > Ranger Plugin
Enable the Ranger Plugin for HDFS,YARN and Hive > Save

Restart Ranger Service
Restart All required components

#Now check the HDFS, YARN and Hive icons in Ranger UI.
[HDFS Ranger Policies]
#login to CLI with a test user.
test 

$ hdfs dfs -mkdir /abc
Here , there is access denied exception

In Ranger UI.
Go to HDFS, select existing policy.
Add the user name and provide appropriate permissions.

$ hdfs dfs -mkdir /abc
This will now create the above dir


Create a new policy for developers
#add group name in the policy and provide the resource path.

Now users inside developers group will have read/write/execute permissions to the specified resource path.

[HIVE Ranger Policies]
Select existing policies/Add new Policy.

Give below information:
Policy Name :
Hive Database : 
Table/UDF : 
Hive Column : 
Audit Logging : Y
Description : 


Specify :
	Group , User , Permission
		
[Permission]
select,update,Create,Drop,Alter,Index,Lock,All


Another Tab in Ranger UI
[Ranger Audit]
Tabs are : Access, Admin, Login Sessions, Plugins
[Access Tab]
The Table consists of various fields :
Event Time, 		User, 	Service Name/Type,	Resource Name/Type, Access Type, Result, Access Enforcer, Client IP
04/24/2018 11:10:10	dev1,	Cluster-Name		/developers/dev1    Write	 Denied  hadoop-acl       172.31.11.22


The above table consists of filters where by each column and values
#Audits can be search by a particular user,specific client IP
#Audits can be search by a Result value as Denied.
This helps to identify all unauthorized access to service in cluster


[Admin Tab]
All the policy related changes are provided in this tabs.

[Login Sessions Tab]
All the users login session information

[Plugins]
All the plugins information of the Rnager.

------------RANGER Installation--------------------------------------------------------------------------------------

https://www.barchart.com/solutions/sample-data

https://www.alibabacloud.com/help/doc-detail/62704.htm
https://blogs.oracle.com/datawarehousing/practical-hdfs-permissions

Keywords of AllDayDevOps
-Bluemix,Quantom Computers
-KPN security app
-DevSecOps
-InfoQ
-www.gocd.org
-git merge and commit
-Canary release
-Feedback Loops
https://developer.ibm.com/iotplatform/resources/cognitive-iot-cookbook/

Hackerrank - Linux
http://www.panix.com/~elflord/unix/bash-tute.html

http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_08_02.html

https://www.thegeekstuff.com/2010/06/bash-if-statement-examples/


---------------------------------------------------------
#important links 
https://community.hortonworks.com/questions/24180/twitter-to-flume-stored-file-in-avro-format.html

What is server logs?
What are the different filters used to check service logs?
Real time issue you faced and how you resolved it?
Yarn Work Flow.
What is auth_to_local parameter?
	=> https://hortonworks.com/blog/fine-tune-your-apache-hadoop-security-settings/
WA00047
Q - Can you define sink group for load balancing in Flume.
		Load balancing Sink Processor
			Load balancing sink processor provides the ability to load-balance flow over multiple sinks. It maintains an indexed list of active sinks on which the load must be distributed. Implementation supports distributing load using either via round_robin or random selection mechanisms. 
			The choice of selection mechanism defaults to round_robin type, but can be overridden via configuration. Custom selection mechanisms are 
			supported via custom classes that inherits from AbstractSinkSelector.		
			When invoked, this selector picks the next sink using its configured selection mechanism and invokes it. For round_robin and random.
			In case the selected sink fails to deliver the event, the processor picks the next available sink via its configured selection mechanism. 
			This implementation does not blacklist the failing sink and instead continues to optimistically attempt every available sink. 
			If all sinks invocations result in failure, the selector propagates the failure to the sink runner.

			a1.sinkgroups = g1
			a1.sinkgroups.g1.sinks = k1 k2							-Space-separated list of sinks that are participating in the group
			a1.sinkgroups.g1.processor.type = load_balance			-The component type name, needs to be load_balance
			a1.sinkgroups.g1.processor.backoff = true				-Should failed sinks be backed off exponentially.
			a1.sinkgroups.g1.processor.selector = random			-Selection mechanism. Must be either round_robin, random or FQCN of custom class that inherits from AbstractSinkSelector
		If backoff is enabled, the sink processor will blacklist sinks that fail, removing them for selection for a given timeout. When the timeout ends, if the sink is still unresponsive 
		timeout is increased exponentially to avoid potentially getting stuck in long waits on unresponsive sinks. With this disabled, in round-robin all the failed sinks load will be 
		passed to the next sink in line and thus not evenly balanced.
	=>	Flume Sink Processors
		Sink groups allow users to group multiple sinks into one entity. 
		Sink processors can be used to provide load balancing capabilities over all sinks inside the group or to achieve fail over from one sink to another in case of temporal failure.

		Property Name	Default	Description
		sinks	–	Space-separated list of sinks that are participating in the group
		processor.type	default	The component type name, needs to be default, failover or load_balance
		Example for agent named a1:

		a1.sinkgroups = g1
		a1.sinkgroups.g1.sinks = k1 k2
		a1.sinkgroups.g1.processor.type = load_balance
		
		
Q - If we want to use Avro Sink 2 properties , how can we define it?
		Configuring a multi agent flow
		To setup a multi-tier flow, you need to have an avro/thrift sink of first hop pointing to avro/thrift source of the next hop. 
		This will result in the first Flume agent forwarding events to the next Flume agent. For example, if you are periodically sending files 
		(1 file per event) using avro client to a local Flume agent, then this local agent can forward it to another agent that has the mounted for storage.
			Weblog agent config:

			# list sources, sinks and channels in the agent
			agent_foo.sources = avro-AppSrv-source
			agent_foo.channels = file-channel
			agent_foo.sinks = avro-forward-sink
			
			# define the flow
			agent_foo.sources.avro-AppSrv-source.channels = file-channel
			agent_foo.sinks.avro-forward-sink.channel = file-channel

			# avro sink properties
			agent_foo.sinks.avro-forward-sink.type = avro
			agent_foo.sinks.avro-forward-sink.hostname = 10.1.1.100
			agent_foo.sinks.avro-forward-sink.port = 10000

			# configure other pieces
			#...
			
			
			HDFS agent config:

			# list sources, sinks and channels in the agent
			agent_foo.sources = avro-collection-source
			agent_foo.sinks = hdfs-sink
			agent_foo.channels = mem-channel

			# define the flow
			agent_foo.sources.avro-collection-source.channels = mem-channel
			agent_foo.sinks.hdfs-sink.channel = mem-channel

			# avro source properties
			agent_foo.sources.avro-collection-source.type = avro
			agent_foo.sources.avro-collection-source.bind = 10.1.1.100
			agent_foo.sources.avro-collection-source.port = 10000
		
			agent_foo.sinks.hdfs-sink.type = hdfs
			agent_foo.sinks.hdfs-sink.hdfs.path = hdfs://namenodeIP/flume/webdata
			# configure other pieces
			#...
			
			Here we link the avro-forward-sink from the weblog agent to the avro-collection-source of the hdfs agent. 
			This will result in the events coming from the external appserver source eventually getting stored in HDFS.
			
Q - Tell me about any Upgrade in your Cluster (CM/CDH)
	To update Cloudera Manager from version 5.X to 5.Y.
		Step1) 	Stop the Cloudera Management Service. If embedded PostgreSQL database is used , Stop all services that are using the embedded database:
				Hive service and all services such as Impala and Hue that use the Hive metastore , Oozie , Sentry.
		Step2)	Stop Cloudera Manager Server, Database, and Agent
				$ sudo service cloudera-scm-server stop
				$ sudo service cloudera-scm-server-db stop
				$ sudo service cloudera-scm-agent stop
		Step3)	Upgrade the JDK on Cloudera Manager Server and Agent Hosts - if necessary
		Step4)	Upgrade Cloudera Manager Server (Packages)
				Find the Cloudera repo file for your distribution by starting at https://archive.cloudera.com/cm5/ and 
				navigating to the directory that matches your operating system.
					For example, for Red Hat or CentOS 6, you would go to https://archive.cloudera.com/cm5/redhat/6/x86_64/cm/. 
						In that directory, find the repo file that contains information including the repository base URL and GPG key. 
						The contents of the cloudera-manager.repo are similar to the following:
						[cloudera-manager]
						# Packages for Cloudera Manager, Version 5, on RHEL or CentOS 6 x86_64
						name=Cloudera Manager
						baseurl=https://archive.cloudera.com/cm5/redhat/6/x86_64/cm/5/
						gpgkey = https://archive.cloudera.com/cm5/redhat/6/x86_64/cm/RPM-GPG-KEY-cloudera 
						gpgcheck = 1
	
					For Ubuntu or Debian systems, go to the appropriate release directory, for example, https://archive.cloudera.com/cm4/debian/wheezy/amd64/cm. 
					The repo file, in this case, cloudera.list, is similar to the following:
						# Packages for Cloudera Manager, Version 5, on Debian 7.0 x86_64
						deb https://archive.cloudera.com/cm5/debian/wheezy/amd64/cm wheezy-cm5 contrib
						deb-src https://archive.cloudera.com/cm5/debian/wheezy/amd64/cm wheezy-cm5 contrib
						The following commands clean cached repository information and update Cloudera Manager components:
						$ sudo apt-get clean $ sudo apt-get update $ sudo apt-get dist-upgrade $ sudo apt-get install cloudera-manager-server cloudera-manager-daemons cloudera-manager-server-db-2 cloudera-manager-agent
						During this process, you may be prompted about your configuration file version:
						Configuration file `/etc/cloudera-scm-agent/config.ini' ==> Modified (by you or by a script) since installation. ==> Package distributor has shipped an updated version. What would you like to do about it ? Your options are: Y or I : install the package maintainer's version N or O : keep your currently-installed version D : show the differences between the versions Z : start a shell to examine the situation The default action is to keep your current version.
						You will receive a similar prompt for /etc/cloudera-scm-server/db.properties. Answer N to both prompts.
		Step5)	Start the Cloudera Manager Server (Packages)
					On the Cloudera Manager Server host (the system on which you installed the cloudera-manager-server package) do the following:

					If you are using the embedded PostgreSQL database for Cloudera Manager, start the database:
					$ sudo service cloudera-scm-server-db start
					Start the Cloudera Manager Server:
					$ sudo service cloudera-scm-server start
					You should see the following:
					Starting cloudera-scm-server:    				[  OK  ]
				Upgrade and Start Cloudera Manager Agent 
					(Packages):
						Log in to the Cloudera Manager Admin Console.
						Upgrade hosts using one of the following methods:
						Cloudera Manager installs Agent software
						Select Yes, I would like to upgrade the Cloudera Manager Agent packages now and click Continue.
						Select the release of the Cloudera Manager Agent to install. Normally, this is the Matched Release for this Cloudera Manager Server. 
						However, if you used a custom repository (instead of archive.cloudera.com) for the Cloudera Manager server, select Custom Repository and 
						provide the required information. The custom repository allows you to use an alternative location, but that location must contain the matched Agent version.
					Manually install Agent software:
						On all cluster hosts except the Cloudera Manager Server host, stop the Agent:
						$ sudo service cloudera-scm-agent stop
						
						In the Cloudera Admin Console, select No, I would like to skip the agent upgrade now and click Continue.
						Copy the appropriate repo file as described in Upgrade Cloudera Manager Server (Packages).
						Run the following commands:
						Operating System	Commands
						RHEL	
						$ sudo yum clean all $ sudo yum upgrade cloudera-manager-server cloudera-manager-daemons cloudera-manager-server-db-2 cloudera-manager-agent
						Ubuntu or Debian
						$ sudo apt-get clean $ sudo apt-get update $ sudo apt-get dist-upgrade $ sudo apt-get install cloudera-manager-agent cloudera-manager-daemons

Q - Why Security Inspector is needed and how to run Security Inspector?
Q - What about TLS/SSL clients.
Q - What are the sevices stopped during the Upgradation of Cloudera Manager
Q - How to take important backup for particular service.
	-Metadata backup
	-Hive Metastore Backup
	-Database backup of services

Q - can you tellme about oneclick Install?  	
Q - While upgrading , if process fails, what to do in such case.
	-If any process is failed during upgrade, and if restart is required, what can be done.
	
Q - Why Rolling Upgrade is required.
Q - Production Cluster

Q - If Datanode HDD is failed, how the data can be checked

Q - What are the Services on the cluster.(HDFS,YARN,Zookeeper,Oozie,Flume,Hive,Sqoop,Hue)
	Why are all of services required.

Q - Tell me about the security in your Cluster (Active Directory)
	=> The Senior Admin had configured, I have configured on Staging cluster 
	
Q - Why are keytab and JCE policies required.
Q - how to map principles to Active Directory.
	https://www.cloudera.com/documentation/enterprise/5-8-x/topics/cdh_sg_kerbprin_to_sn.html

42:00 - 1:19:30
Q- How kerberos works ?

----------------------------------------------------------------------
Practical information : 
Download Data Links : https://geonames.usgs.gov/domestic/download_data.htm
Create a cluster with security enable and run jobs with Fair Scheduler to check variours fair-scheduler.xml properties.
Q - What is difference between CM Server and CM Agent
Q - Performance Tuning (Speculative Execution of Mappers/Reducers)
Q - Map reduce Tuning 
Q - What is ulimit is used for
Q - Best Practices for Mapreduce configuration.
http://blog.cloudera.com/blog/2009/12/7-tips-for-improving-mapreduce-performance/
	=>	Tip 1) Configure your cluster correctly
			The first step to optimizing your MapReduce performance is to make sure your cluster configuration has been tuned.
			Make sure the mounts you’re using for DFS and MapReduce storage have been mounted with the noatime option. 
			This disables access time tracking and can improve IO performance.
			Make sure you’ve configured mapred.local.dir and dfs.data.dir to point to one directory on each of your disks to ensure that all of your IO capacity is used. 
			Run iostat -dx 5 from the sysstat package while the cluster is loaded to make sure each disk shows utilization.
			Monitor and graph swap usage and network usage with software like Ganglia. Monitoring Hadoop metrics in Ganglia is also a good idea. 
			If you see swap being used, reduce the amount of RAM allocated to each task in mapred.child.java.opts
		Tip 2) Use LZO Compression
			Whenever a job needs to output a significant amount of data, LZO compression can also increase performance on the output side. 
			Since writes are replicated 3x by default, each GB of output data you save will save 3GB of disk writes.In order to enable LZO compression, 
			check out our recent guest blog from Twitter. Be sure to set mapred.compress.map.output to true.
	=>	https://www.howtoforge.com/reducing-disk-io-by-mounting-partitions-with-noatime
	
Q - What is tuning required in Map Reduce .
Q - How is HA configured and working of Namenode and Resource Manager HA.
Q - What are the issues faced during CM Upgrade
Q - How scheduler works (Fair Schedular in Cloudera) along with parameters and property files.
	Configuring the Fair Scheduler
	The Fair Scheduler is the Cloudera recommended scheduler option. The Fair Scheduler controls how resources are allocated to pools (or queues) and 
	how jobs are assigned to pools. Jobs can also be explicitly submitted to pools; to submit an job to a specific pool, you specify the mapreduce.job.queuename property.
	Queues enable the scheduler to allocate resources. All cluster users are assigned to a queue named “default.”
	The Fair Scheduler relies on resource queues or pools that are built in a hierarchical fashion. All queues descend from the same ancestor named the root queue. 
	The descendent queues are called leaf queues, on which the actual applications are scheduled. Leaf queues can have more levels of child queues of their own.
	
	Configuring the Fair Scheduler
	You enable the Fair Scheduler in your cluster by specifying the configuration parameter yarn.resourcemanager.scheduler.class in the yarn-site.xml file as follows:
	<property>
	<name>yarn.resourcemanager.scheduler.class</name>
	<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
	</property>
	Also in the yarn-site.xml file, specify the location of the allocation file (fair-scheduler.xml) for the Fair Scheduler, as shown here:
	<property>
	<name>yarn.scheduler.fair.allocation.file</name>
	<value>/etc/hadoop/conf/fair-scheduler.xml</value>
	</property>
	
	Configuring the Queues in the fair.scheduler.xml File
		The fair-scheduler.xml file consists of various types of elements, such as the queue, user and queuePlacementPolicy elements. 
		You can specify values for various queue, user and scheduling policy related elements in the fairscheduler.xml file.
		Note :
			The user element determines how the individual users can behave, and there’s only a
			single property named 'maxRunningApps' that you can configure under this element.
	
		The userMaxAppsDefault element specifies the default maximum number of running applications for a user if you don’t explicitly specify a limit yourself. 
		You can control the maximum number of applications that a queue can run by setting the limit with the queueMaxAppsDefault element.
			
	
	http://archive.cloudera.com/cdh5/cdh/5/hadoop/hadoop-yarn/hadoop-yarn-site/FairScheduler.html#Configuration
	To be continued...

Pools have policies for preempting running jobs, so that when there is contention for resources, jobs that have high priority or have been waiting a long time to run are allowed to run.

Fair Scheduler configuration is maintained in two files: yarn-site.xml and fair-scheduler.xml. Detailed information on the available properties is available at Fair Scheduler Configuration. When you change the contents of yarn-site.xml, you must restart the YARN service.
Q - what are commands with dfsadmin?
	dfsadmin Runs a HDFS dfsadmin client.

	Usage: hadoop dfsadmin [GENERIC_OPTIONS] [-report] [-safemode enter | leave | get | wait] [-refreshNodes] [-finalizeUpgrade] [-upgradeProgress status | details | force] [-metasave filename] [-setQuota <quota> <dirname>...<dirname>] [-clrQuota <dirname>...<dirname>] [-help [cmd]]

	COMMAND_OPTION	Description
	-report	Reports basic filesystem information and statistics.
	-safemode enter | leave | get | wait	Safe mode maintenance command. Safe mode is a Namenode state in which it 
	1. does not accept changes to the name space (read-only) 
	2. does not replicate or delete blocks. 
	Safe mode is entered automatically at Namenode startup, and leaves safe mode automatically when the configured minimum percentage of blocks satisfies the minimum replication condition. Safe mode can also be entered manually, but then it can only be turned off manually as well.
	-refreshNodes	Re-read the hosts and exclude files to update the set of Datanodes that are allowed to connect to the Namenode and those that should be decommissioned or recommissioned.
	-finalizeUpgrade	Finalize upgrade of HDFS. Datanodes delete their previous version working directories, followed by Namenode doing the same. This completes the upgrade process.
	-upgradeProgress status | details | force	Request current distributed upgrade status, a detailed status or force the upgrade to proceed.
	-metasave filename	Save Namenode's primary data structures to <filename> in the directory specified by hadoop.log.dir property. <filename> will contain one line for each of the following 
	1. Datanodes heart beating with Namenode
	2. Blocks waiting to be replicated
	3. Blocks currrently being replicated
	4. Blocks waiting to be deleted
	-setQuota <quota> <dirname>...<dirname>	Set the quota <quota> for each directory <dirname>. The directory quota is a long integer that puts a hard limit on the number of names in the directory tree.
	Best effort for the directory, with faults reported if
	1. N is not a positive integer, or
	2. user is not an administrator, or
	3. the directory does not exist or is a file, or
	4. the directory would immediately exceed the new quota.
	-clrQuota <dirname>...<dirname>	Clear the quota for each directory <dirname>.
	Best effort for the directory. with fault reported if
	1. the directory does not exist or is a file, or
	2. user is not an administrator.
	It does not fault if the directory has no quota.
	-help [cmd]	Displays help for the given command or all commands if none is specified.




Info - 	After security is enabled, when data is encrypted , when we try to delete any file, the files were not being deleted.
		Use -skipTrash command
		as trash does not accept encrypted files.

Q - Can you explain Hive Query Process? Hive Client . Hive Server , Hive Metastore
Q - Who is going to take care of Host Inspector ? =>(CM agent or server)

------------------------------------------------------------------------------
[Apache Hadoop CTS.txt]
Apache Hadoop Users:
Google uses Hadoop :
Google Earth Uses Hadoop
-to store Browsing history
analysis,to make recommendations


Amazon uses hadoop
-to store purchase history.
-Purchase history->recommend products
-Page history is also saved.
-Page history indicates what products are popular.
-Popular products are suggested to the customer.

Paypal 
-to ensure data security
-A large number of sales records can be saved  and analysed.
-Hadoop finds patterns in the data , perhaps anomalies.
-Data anomalies help find and prevent fraud.


Sources of Big Data:
-Social Network
-Sensors Log


Overview/Description 
Apache Hadoop is a set of algorithms for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. This course will introduce the basic concepts of cloud computing using Apache Hadoop, cloud computing, Big Data, and the development tools applied. 


Target Audience 
This path is designed for developers, managers, database developers, and anyone interested in learning the basics of Hadoop, or cloud computing in general. 


Expected Duration 
1 Hours, 59 minutes 

Prerequisites 
None 

objectives 
Course Introduction

Basics of Hadoop 

•describe the basics of Hadoop

Users of Hadoop 

•identify the major users of Hadoop, the end-user application, and the result

The Four Vs of Big Data 

•identify the characteristics of Big Data

Sources of Big Data 

•compare and contrast the traditional data sources and Big Data sources

Clustering 

•describe the clustering and distributed computing concepts of Hadoop

Introduction to Server Farms 

•specify low cost commodity servers in Big Data and its configurations as nodes in small and large scale Hadoop installations

Hadoop Installation Considerations 

•describe Hadoop installation requirements

Testing your Hadoop Installation 

•troubleshoot Hadoop installation issues

Your First Hadoop Cluster 

•configure Hadoop installation

Hadoop Distributions 

•identify the features of third party Hadoop distributions

Hadoop Ecosystem 

•describe the creation and evolution of Hadoop and its related projects

The Role of YARN 

•describe the use of YARN in Hadoop cluster management

Overview of Hadoop, Storage, MapReduce, Pig, and Hive 

•describe the components and functions of Hadoop

Understanding Data 

•compare and contrast the different types of Hadoop data

Types of NoSQL Databases 

•describe the four different types of cloud databases in NoSQL Databases
1) Key-value Stores
2)Column Family Stores
3)Document Databases
4)Graph Databases

Introduction to the Hadoop Distributed File System 

•describe the basics of the Hadoop Distributed File System

Interacting with the HDFS 

•describe HDFS and basic HDFS navigation operations

File Operations within the HDFS 

•perform file operations such as add and delete within HDFS

The MapReduce Principles, Mappers, and Reducers 

•describe the basic principles of MapReduce and general mapping issues

Using MapReduce with Pig and Hive 

•specify the use of Pig and Hive in Hadoop Map Reduce jobs

Introduction to the MapReduce Life Cycle 

•describe the use of MapReduce, MapReduce lifecycle, job client, job tracker, task tracker, map tasks, and reduce tasks

Understanding the MapReduce Data Flow 

•describe Hadoop MapReduce handles, data processes data, and vocabulary of the MapReduce dataflow process

Subdividing Data 

•describe the process of mapping and reducing

Exercise: Apache Hadoop Basics



Course Number df_ahmr_a01_it_enus 


Fileinputformat
Fetch the job -> NO

------------------------------------------------------------------------------------------------------------------------
[Centurysoft-Profile.txt]
Title	Hadoop Admin
Location	Pune
Job Information	
Responsible for implementation and on going administration of Hadoop infrastructure.

Aligning with the systems engineering team to propose and deploy new hardware and software environments required for Hadoop and to expand existing environments.
Working with data delivery teams to setup new Hadoop users. This job includes setting up Linux users, setting up Kerberos principals and testing HDFS, Hive, Pig and MapReduce access for the new users.
Cluster maintenance as well as creation and removal of nodes using tools like Ganglia, Nagios, Hortonworks Ambari, Cloudera Manager Enterprise.
Performance tuning of Hadoop clusters and Hadoop MapReduce routines.
Screen Hadoop cluster job performances and capacity planning
Monitor Hadoop cluster connectivity and security
Manage and review Hadoop log files.
File system management and monitoring.
HDFS support and maintenance.
Diligently teaming with the infrastructure, network, database, application and business intelligence teams to guarantee high data quality and availability.
Collaborating with application teams to install operating system and Hadoop updates, patches, version upgrades when required.
Data modelling, design & implementation based on recognized standards.
Software installation and configuration.
Database backup and recovery.
Database connectivity and security.
Performance monitoring and tuning.
Disk space management.
Software patches and upgrades.
Automate manual tasks.
------------------------------------------------------------------------------------------------------------------------
[Day-to-day.txt]
Day to Day Activities-Abhishek
1) Perform Health checkup using WebUI for namenode and resource Manager
2) Manage Alert Publisher in Cloudera Manager
	-Under Cloudera Management Service
	-Logs related to particular clouder management service are stored under : 
		/var/log/cloudera-scm-agent/cloudera-scm-alertpublisher  -> Alert Publisher
		/var/log/cloudera-scm-agent/cloudera-scm-eventserver  -> Event Server
	-Cloudera Manger installer logs are stored in /var/log/cloudera-scm-agent/cloudera-manager-installer
	
	
3) Generate Cluster health report from CM, using Report Manager.
4) Backups
	-Namenode Metadata backups
	-Hive Metastore Backup
	-Job Output backups on S3

5)Creating Custom Alerts
	-Check if Jobs are pending on the cluster.
	-Go to YARN > status page
	-Create trigger
	-in expression tab :
		if(select pending_containers_cumulative where category=YARN.POOL and servicename=SERVICENAME
		   and queueName=root and last(pending_containers_cumulative) >10) DO health:bad
	To test the above trigger, run a job(....jar pi 50 50) on hadoop cluster.
	Check the YARN status in ->Cluster>Yarn Applications
	Check the YARN status in ->Cluster>Dynamic Resource Pools

6) Configure the Alert Publisher.
	-Go to Cloudera Management Service
	-Alert Publisher
	-Mail SMTP details

	To check the email was sent , Diagnostics>Logs>Search 'email'
7) Generate Custom report.
	Go to Cluster>report>Custom Reports

------------------------------------------------------------------------------------------
[Upgrade session]
Hadoop Upgrade Session - Ajmal
-Cluster has EMC OneFS for OS on VM
-HDP version 2.3.2 , not working basic fs -ls command.

-Do upgrade on 

Interview info
Introduction,Project,Experience-Judges communication and knowledge
Cluster Planning (done or a part of Solutions Architect Team)
Interview Decider {Upgrade cluster,Security}



-Database Backup
-Client Confirmation - Release Notes (from 5.x to 5.y)
	Note : in Cloudera 5.1, HA was not present, HA is present after 5.2
Why was the cluster upgraded from 5.1/5.2 to 5.x?
-Maintenance Mode => Suppress Alerts , this mode not necessary for Upgrade
-Services and Cluster Health Check
-storage -> available should be min 3GB.
-Packages/Parcels 
	Note :
	Parcels : When cluster has internet access.
	Packages : When cluster has proxy(internet access),download tar ball in proxy node and give repo path on other nodes.
-Online/offline
-OS Compability


Hortonworks Upgrade types
Rolling Upgrade -> No Downtime (takes time)
Express Upgrade -> Downtime required (depends)

-----------------------------------------------------------------------
[HDP info]
https://hortonworks.com/tutorial/manage-files-on-hdfs-via-cli-ambari-files-view/section/1/
https://onlinehelp.tableau.com/current/pro/desktop/en-us/examples_hortonworkshadoop.html
[Hive]
Q) Hive Query Process

[Upgrade]
Q) Role of Cloudera Manager Agent in Upgrade Process

[Security]
Q) What is delegation ticket
Users in a Hadoop cluster authenticate themselves to the NameNode using their Kerberos credentials. However, once the user is authenticated, each job subsequently submitted must also be checked to ensure it comes from an authenticated user. Since there could be a time gap between a job being submitted and the job being executed, during which the user could have logged off, user credentials are passed to the NameNode using delegation tokens that can be used for authentication in the future.

Delegation tokens are a secret key shared with the NameNode, that can be used to impersonate a user to get a job executed. While these tokens can be renewed, new tokens can only be obtained by clients authenticating to the NameNode using Kerberos credentials. By default, delegation tokens are only valid for a day. However, since jobs can last longer than a day, each token specifies a JobTracker as a renewer which is allowed to renew the delegation token once a day, until the job completes, or for a maximum period of 7 days. When the job is complete, the JobTracker requests the NameNode to cancel the delegation token.
Q) How to specify local MIT hostname


[HA]
Q) How the process works when active namenode is down?


[Performance Tuning]
Q) How to increase the heap size of JVM for map/reduce task?
https://discuss.pivotal.io/hc/en-us/articles/201462036-MapReduce-YARN-Memory-Parameters
Commission and Decommission a host using Cloudera Manager.
-Adding a new Node/Commission a host using Cloudera Manager.
	Prerequisites for adding a new host in Cluster.
	-SELinux,vm.swappiness,Transparent Huge pages,IpTables off,NTP configured.
	Go to hosts > Add New Hosts to Cluster.
	Provide FQDN and search the host.
	Install the JDK and Cloudera Manager Agent, this will same as version on other nodes.
	After above step, the CDH Parcels will distributed and Activated on the new nodes.
	After above step, host inspector gives brief information on various services e.g swappiness.
	Cloudera manager provides the commands to be executed for prerequisites on the newly added node.
	Now the new nodes is added in the cluster, but there are no roles assigned to the new node.



-Decommission a host using Cloudera Manager.
	Stopping and decomminssioning all roles running on that particular node.
	Applies to HDFS,datanode,Nodemanager,YARN,HBase Region Service.
	if host has any service other than above , then we need to manually decommission the service/roles.
	Once all services/roles are stopped, then host can be decommissioned.
	Go to Hosts>Instances>select any instance >Actions > Hosts Decommission.
	
	The status can be checked on Namenode WebUI.

	Further if the node needs to be removed from the cluster.
	Go to Hosts>Instances>select any instance >Actions > Remove host from cluster.

	Further if the node needs to be removed from the Cloudera Manager
	Go to Hosts>Instances>select any instance >Actions > Remove host from Cloudera Manager.


-Using snapshot using CM to Back Up data
	-Enable a directory for snapshot.
	-Go to HDFS > File Browser > Particular Directory > Enable Snapshot 
	-Snapshot Policy can be created to create a snapshot on regular intervals.
	-To create a Snapshot Policy, go to Main Menu > Backups > Snapshots > Create snapshot Policy.
	-Select particukar service to take snapshot of (HDFS, HBase , Hive)
	-Select daily,weekly,monthly as per requirement to enable the Snapshot Policy.
	
	-To restore files from snapshot, Go to HDFS > File Browser > Directory > Restore Directory from Snapshot>Select snapshot from dropdown list>Restore.
	--------------------------------------------------------------------------------------------------------------
	
[L1]
L1
1) Tell me about yourself, expertise
	1a) did you start with hadoop or any other technology.
2) In your Hadoop Project, what are your Day to Day activities
	2a) Why do you need to balance your cluster.
3) which Hadoop distribution is used in your project
4) If you are planning a cluster ,What is the Cloudera's basic recommendation for Master and workers or (Service nodes and worker nodes)
5) Explain about Production Cluster.
	5a)is it inhouse deployment or Cloud deployment?
6) Why do you need 3 Zookeeper
	6a) How does zookeeper communicate, if in case active NN goes down, how does zookeeper know , if there is fluctuation happened, 
		and how does it resolve it, what strategy it uses
7) In Cloudera Manager, if there is any issue , how alerts is configured for what services.
8) If there is any issue found in the cluster, where do you navigate to search logs (e.g if flume service goes down, where you go to check logs).
9) How is your process of work, how are your issues tracked, which ticketing tool .
10) How many total cluster do you have.
11) Do you what is maintenance mode in Cloudera.
12) commands
	12a) To enter safemode
	12b) while copying the file from local to hdfs , what exactly happens in background, while copying data is there any map reduce job that runs?
	12c) Filesystem check commands
13) Linux commands
	13a)What is command is used to find memory
	13b)What is command to check permission, created/accessed time for particular directory.
	
Suggesstions.
-Go through Cloudera Manager Architecture
-Navigation
-Custer Capacity
-HA
-Impala, Hive architecture, Flume Architecture
-Yarn 
-------------------------------------------------------------------------------------------
[MIT Kerberos]
Prerequisites :
Create a gateway on a node.
Gateway : JVM layer (top of JVM)
Edge : Linux Layer (below JVM)
Non-kerberised cluster does not need Gateway.
Hive : Goes on each node and process


Canary error
	->If Kerberos is installed and JCE policy files are not present, then Canary error occurs
-------------------------------------------------------------------------------------------
[QA-set]

Q) Daily Task?
Q) Can we Upgrade CDH 5.5 to 5.10 or any other latest version directly?
A) https://www.cloudera.com/documentation/enterprise/5-10-x/topics/cdh_ig_upgrade_to_latest.html
Q) How data is written on HDFS?
A) https://data-flair.training/blogs/hadoop-hdfs-data-read-and-write-operations/

Q) Mapreduce Job Flow in YARN?

Q) What is Disk Spill?
A) 	http://www.hadooplessons.info/2015/10/intermediate-data-spill-in-mapreduce.html
	http://data-flair.training/forums/topic/explain-the-process-of-spilling-in-mapreduce

Q) How will you troubleshoot a job (e.g Hive or Pig Job).
https://acadgild.com/blog/troubleshoot-hive-pig-errors/

Q) Have you deployed Kerberos?

https://ambari.apache.org/1.2.5/installing-hadoop-using-ambari/content/ambari-kerb-1-4.html

Q) If you are authenticating a user for a particular service using AD, then why are you using kerberos?

Q) What is headless keytab and how to generate it?

Q) How to increase ticket lifetime ?

Q) How many number of buckets you have (S3) ?

Q) ANN and SNN both are down. SNN does not have the updated metadata and unfortunately we are not making any other 
backups to restore the ANN. How will you bring up the Filesystem now?

A) Check all the fsimage and edits files in both ANN and SNN and also on all Journal Nodes. Restore from latest file found.

Q) What is OpenSSL ? Have you configured it and how to import certificate.

Q) In mapreduce job, why mappers output is stored in Local File System.

Q) How to increase heap size of a particular hive query?
	Setting Parameters on a Per-Query Basis
	Turn on this optimization feature on a per-query basis by setting these parameters in the query code with the Hive SET command:

set hive.blobstore.use.blobstore.as.scratchdir=false
set hive.exec.copyfile.maxsize=1099511627776;
A) 	https://community.hortonworks.com/content/supportkb/48788/i-am-seeing-outofmemory-errors-when-i-run-a-hive-q.html
	https://www.cloudera.com/documentation/enterprise/5-9-x/topics/admin_hive_tuning.html
---------------------------------------------------------------------
[Re-L1]
1) Tell me something about you. (need to have a good flow for points)
2) Tell me your Day-to-day activities (confidently say 12 activities)
	1>To monitor the Health of the cluster using WebUI/CLI
	-Using WebUI
	???
	???
	???
	-Using CLI
		$ hdfs fsck /
			.........Status: HEALTHY........
			Total size: 146228151045063 B
			Total dirs: 28152
			Total files: 1077255
			Total symlinks: 0
			Total blocks (validated): 1170946 (avg. block size 124880354 B)
			Minimally replicated blocks: 1170946 (100.0 %)
			Over-replicated blocks: 0 (0.0 %)
			Under-replicated blocks: 0 (0.0 %)
			Mis-replicated blocks: 0 (0.0 %)
			Default replication factor: 3
			Average block replication: 2.9950006
			Corrupt blocks: 0
			Missing replicas: 0 (0.0 %)
			Number of data-nodes: 18
			Number of racks: 1
			FSCK ended at Fri May 01 09:45:41 CDT 2015 in 18069 milliseconds
			The filesystem under path '/' is HEALTHY
	2>Balancing HDFS
		The best way to figure out whether the frequency of balancing is okay is to issue the hdfs dfsadmin –report command and check how closely the nodes are balanced
		in terms of HDFS data.
		# sudo -u hdfs hdfs dfsadmin -report | cat <(echo "Name: Total") - | grep '^\(Name\|Total\|DFSUsed\)' | tr '\n' '\t' | sed -e 's/\(Name\)/\n\1/g' | sort --field-separator=:--key=5,5n
		Name: Total DFS Used: 2141803616206687 (1.90 PB) DFS Used%: 59.08%
		Name: 10.192.0.106:50010 (hadoop03.example.com) DFS Used: 46065068713731 (41.90TB) DFS Used%: 50.25%
		Name: 10.192.0.108:50010 (hadoop05.example.com) DFS Used: 46077744360551 (41.91TB) DFS Used%: 50.26%
		Name: 10.192.0.110:50010 (hadoop07.example.com) DFS Used: 46176056279144 (42.00TB) DFS Used%: 50.37%
			
		In this case, the DFS Used percentage is pretty much the same on most nodes, so no balancing is required. If you run the balancer now, there’s no harm—it wraps up very
		quickly since there’s not much for it to do.Sometimes a balancer job gets stuck, or is just too slow. You can restart the balancer in such a situation. 
		First find out the process ID (PID) of the running balancer process with the following command:
		# ps aux | grep '\-Dproc_balancer' | grep -v grep
		Once you find the PID of the balancer job, kill it, and also remove the balancer lock file, as shown here:
		# rm /tmp/hdfs-balancer.lock
		
		
		hdfs balancer
          [-policy <policy>]
          [-threshold <threshold>]                                       	Percentage of disk capacity. This overwrites the default threshold.
          [-exclude [-f <hosts-file> | <comma-separated list of hosts>]]	Excludes the specified datanodes from being balanced by the balancer.
          [-include [-f <hosts-file> | <comma-separated list of hosts>]]	Includes only the specified datanodes to be balanced by the balancer.
          [-source [-f <hosts-file> | <comma-separated list of hosts>]]		Pick only the specified datanodes as source nodes.
          [-blockpools <comma-separated list of blockpool ids>]				The balancer will only run on blockpools included in this list.
          [-idleiterations <idleiterations>]								Maximum number of idle iterations before exit. This overwrites the default idleiterations(5).
          [-runDuringUpgrade]												Display the tool usage and help information and exit.
		  		
	3> Cluster Data backups:
		
		-HDFS Metadata Backup
			Backing Up the HDFS Metadata on the NameNode
			Backing Up the Metadata Using the -fetchImage Command
				a. Place the active NameNode in safe mode so HDFS is in a read-only state when
					you back up its metadata. Also verify that the cluster is in safe mode.
				# su hdfs
				bash-3.2$ hdfs dfsadmin -fs hdfs://hadoop011node01.example.com -safemode enter
				Safe mode is ON
				$ hdfs dfsadmin -fs hdfs://hadoop11node01.example.com -safemode get
				Safe mode is ON
				$
				b. Run the –saveNamespace command, which will save the current in-memory
				image file to a new fsimage file and reset the edits file, as well:
				$ hdfs dfsadmin -fs hdfs://hadoop011node01.example.com -saveNamespace
				Save namespace successful					
				c. Issue the -fetchImage command to back up the HDFS metadata to disk.
					$ mkdir /tmp/Backups_node01
					$ hdfs dfsadmin -fs hdfs://hadoop011node01.example.com -fetchImage /tmp/Backups_node01
						16/08/26 11:41:01 INFO namenode.TransferFsImage: Opening connection to
						http://bdaolc011node01.sabre.com:50070/imagetransfer?getimage=1&txid=latest
						16/08/26 11:41:01 INFO namenode.TransferFsImage: Image Transfer timeout
						configured to 60000 milliseconds
						16/08/26 11:41:04 INFO namenode.TransferFsImage: Transfer took 3.45s at 72463.81 KB/s
				d. Verify the metadata backup.
					$ ls -altr /tmp/Backups_node01
					total 250552
					drwxrwxrwt 22 root root 4096 Aug 26 11:40 ..
					-rw-r--r-- 1 hdfs hadoop 256297032 Aug 26 11:41 fsimage_0000000000490831699
					drwxr-xr-x 2 hdfs hadoop 4096 Aug 26 11:41 .
			
				e. Take the active NameNode out of safe mode.
					$ hdfs dfsadmin -fs hdfs://hadoop011node01.example.com -safemode leave
					Safe mode is OFF
		-HDFS Replicsation
		-Backing Up the Metastore Databases
			Assuming you’re using the MySQL database as the repository for the metastores of
			the Hadoop components, issue the mysqldump command to back up the MySQL database.
			Here’s the syntax:
			$ mysqldump –h hostname –u username –p password database > /backup/mysql/mysql_backup.sql
		-Hive Replication
			Hive metastore 9083 thrift Service for accessing Metadata
			hive.metastore.uris
		-Cluster Backup using Distcp
	-Manager Cloudera Manager Users.
	-Create snapshot 
	-Configure Trash 
	-Setting Users Space Quota, number of file quota
Suggesstions : In real time scenario , 12 to 15 daytoday activities.
3) Do you know what are under-replicated blocks
                get the full details of the files which are causing your problem using
                $hdfs fsck / -files -blocks -locations
               
                To Fix under-replicated blocks in HDFS, below is quick instruction to use:
               
                su - <$hdfs_user>
               #the below command will list all under replicated files to '/tmp/under_replicated_files'
                $ hdfs fsck / | grep 'Under replicated' | awk -F':' '{print $1}' >> /tmp/under_replicated_files
 
                $ for hdfsfile in `cat /tmp/under_replicated_files`; do echo "Fixing $hdfsfile :" ;  hadoop fs -setrep 3 $hdfsfile;done
                => Namenode WebUI , blocks are hyperlink to get more information.      
                3a) How will you monitor the under-replicated blocks,as to which blocks are under replicated (using CLI and CM)
                                =>CLI
                                	$ hdfs fsck -list-corruptfileblocks
                                             The filesystem under path '/' has 2114 CORRUPT files
                                If files are not important can be deleted by using the command
                                                $ hdfs fsck / -delete
4) if there is a job submitted and it is working slow/hung.How will you monitor the job
                ->Resource Manager UI/YARN Application UI
                                Check the number of mappers and reducers available, or job queues.
5) Have you done Commissioning/De-Commissioning
                5a) What is Pre-requisites of Commissioning/De-Commissioning
		Commission and Decommission a host using Cloudera Manager.
			-Adding a new Node/Commission a host using Cloudera Manager.
        	        Prerequisites for adding a new host in Cluster.
        	        -SELinux,vm.swappiness,Transparent Huge pages,IpTables off,NTP configured.
        	        Go to hosts > Add New Hosts to Cluster.
        	        Provide FQDN and search the host.
        	        Install the JDK and Cloudera Manager Agent, this will be same as version on other nodes.
        	        After above step, the CDH Parcels will be distributed and Activated on the new nodes.
        	        After above step, host inspector gives brief information on various services e.g swappiness.
        	        Cloudera manager provides the commands to be executed for prerequisites on the newly added node.
        	        Now the new nodes is added in the cluster, but there are no roles assigned to the new node. 
		-Decommission a host using Cloudera Manager.
                	Stopping and decomminssioning all roles running on that particular node.
                	Applies to HDFS,datanode,Nodemanager,YARN,HBase Region Service.
                	if host has any service other than above , then we need to manually decommission the service/roles.
                	Once all services/roles are stopped, then host can be decommissioned.
                	Go to Hosts>Instances>select any instance >Actions > Hosts Decommission.
               	
                	The status can be checked on Namenode WebUI.
 	
                	Further if the node needs to be removed from the cluster.
                	Go to Hosts>Instances>select any instance >Actions > Remove host from cluster.
 	
                	Further if the node needs to be removed from the Cloudera Manager
                	Go to Hosts>Instances>select any instance >Actions > Remove host from Cloudera Manager.

                =>Cluster should be in maintenance mode.
                =>Using the Cloudera manager explain steps.
6) Tell me about your Capacity Planning
                                Daily Data : 10GB , with repliation 30GB
                                Monthly Data with replication : 900GB
                                Yearly Data : 10.5 TB DFS
                                30% Non-DFS to be added to above value : 10.5+3.2 = 13.7TB
                                10% buffer space i.e 1TB , so total data for 1 year is 16TB
                               
                                Total Number of Nodes in Cluster : 13
                                Master Nodes : 4 (NN,Standby NN,RM , Standby RM)
                                8 DN - 2TB storage each
                                1 Edge Node : Cloudera Manager
                               
                6a) Why do you require Non-DFS space in cluster
                                =>Non DFS used is any data in the filesystem of the data node(s) that isn't in dfs.data.dirs.
                                This would include log files, mapreduce shuffle output and local copies of data files (if you put them on a data node).
                                Use du or a similar tool to see whats taking up the space in your filesystem.
7) Tell me about your cluster configuration.
8) What is HA?
                a)How to configure HA in Cloudera Manager.
                                =>Go to the HDFS service>Select Actions > Enable High Availability.
                                Give any appropriate NameService Name.
                                -Now Assign Role i.e Select Namenode Host for Standby Host
                                -Select Journal Nodes (recommended 3 or more)should be on NN,Standby NN and RM node, click on continue.
                                Specify the below property :
                                ->dfs.namenode.name.dir : can be same as previous for both nodes (node01,node02).
                                ->dfs.journalnodes.edits.dir : specify directory name (/dfs/edits) for all 3 nodes.
                                Click on Continue
                                Below are summary steps that Cloudera Manager does to Enable High Availability:
                                -Check if namenode directory is empty/present/clear on new Standby NameNode
                                -Check if dfs.journalnodes.edits.dir directory is empty/present/clear on all journal nodes.
                               
                                Extra Options
                                A screen showing the hosts that are eligible to run a standby NameNode and the JournalNodes displays.
                b)Why there is need for 3 Zookeeper in HA?
			=>Apache ZooKeeper is a distributed coordination service. It is a framework that can be used to build distributed applications 
			by providing a set of services such as a name service, locking,synchronization, configuration management, and leader election 
			services. These services are explained as follows:
			=>Zookeeper requires that you have a quorum of servers up, where quorum is ceil(N/2). For a 3 server ensemble, 
			that means 2 servers must be up at any time,for a 5 server ensemble, 3 servers need to be up at any time.
			9) Tell me the YARN job Flow.
11) Application Master is inside container OR container is inside Application Master?                      
                =>When Job is submitted, RM contacts NM, to open a container, Application Master executes inside the container.
12) What happens to the application master/container when a job is submitted/completed.
13) Do you know about Trash. Have you configured it (CLI/CM)? What are the steps to configure in CM and using CLI?
                => Configuring HDFS Trash
                                The Hadoop trash feature helps prevent accidental deletion of files and directories. 
								When you delete a file in HDFS,the file is not immediately expelled from HDFS. Deleted 
								files are first moved to the /user/<username>/.Trash/Current directory,
                                with their original filesystem path being preserved. After a user-configurable period of time 
								(fs.trash.interval),a process known as trash checkpointing renames the Current directory to the current timestamp, that is, /user/<username>/.Trash/<timestamp>.
                                The checkpointing process also checks the rest of the .Trash directory for any existing timestamp directories and removes them from HDFS permanently.
                                You can restore files and directories in the trash simply by moving them to a location outside the .Trash directory.
                               
                                Important:
                                The trash feature is disabled by default. Cloudera recommends that you enable it on all production clusters.
                                The trash feature works by default only for files and directories deleted using the Hadoop shell.
                                Files or directories deleted programmatically using other interfaces (WebHDFS or the Java APIs, for example) are not moved to trash,
                                even if trash is enabled, unless the program has implemented a call to the trash functionality.
                                (Hue, for example, implements trash as of CDH 4.4.).Users can bypass trash when deleting files using the shell by specifying
                                the -skipTrash option to the hadoop fs -rm -r command. This can be useful when it is necessary to delete files that are too large
                                for the user's quota.
                               
                => Configuring HDFS Trash Using Cloudera Manager
                                Enabling and Disabling Trash
                                                Go to the HDFS service > Configuration tab >Select Scope > Gateway >Select or clear the Use Trash checkbox.
                                                If more than one role group applies to this configuration, edit the value for the appropriate role group.
                                                See Modifying Configuration Properties Using Cloudera Manager.
                                                Click Save Changes to commit the changes.Restart the cluster and deploy the cluster client configuration
                                Setting the Trash Interval
                                                Go to the HDFS service >Configuration tab > Select Scope > NameNode.
                                                Specify the Filesystem Trash Interval property, which controls the number of minutes after which a trash checkpoint
                                                directory is deleted and the number of minutes between trash checkpoints.
                                                For example, to enable trash so that deleted files are deleted after 24 hours,set the value of the Filesystem Trash Interval property to 1440.
                                                Note: The trash interval is measured from the point at which the files are moved to trash,
                                                not from the last time the files were modified.If more than one role group applies to this configuration,
                                                edit the value for the appropriate role group. See Modifying Configuration Properties Using Cloudera Manager.
                                                Click Save Changes to commit the changes.Restart all NameNodes.
                                               
                                                fs.trash.interval -> Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled.
                                                fs.trash.checkpoint.interval ->    Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval.
                                                                                                                                                                                Every time the checkpointer runs it creates a new checkpoint out of current and removes checkpoints
                                                                                                                                                                                created more than fs.trash.interval minutes ago.
14) What will happen if your Cloudera Manager goes down?
                => Login CM-node and troubleshoot in CLI to check if Cloudera Manager Server is running.
                                Check if cloudera-scm-server is running on port (7180,7182)
15) What is the difference between JobID and ApplicationID?
                =>           If you use YARN version of kill command, the resource manager kills the AM without AM's knowledge so it will not generate history.
                                If you use hadoop job -kill job_id the killing process goes through the AM and it will allow to generate history.
                                In terms of YARN, the programs that are being run on a cluster are called applications. In terms of MapReduce they are called jobs.
                                So, if you are running MapReduce on YARN, job and application are the same thing (if you take a close look, job ids and application ids are the same).                  
                                MapReduce job consists of several tasks (they could be either map or reduce tasks). If a task fails, it is launched again on another node.
                                Those are task attempts.
 
Container is a YARN term. This is a unit of resource allocation. For example, MapReduce task would be run in a single container.
 
16) Tell me in brief about your project.
                Our project focuses on log aggregation of users Swipe-In/Swipe Out Data for a day, for real time analysis of an employee 
				in the company.Sensor logs are generated and are dumped into hdfs using flume for analysis, the analysed data is used 
				to help employee track time and the same is used for client billing. We have RDBMS data ingestion on month ends , 
				to get billing information to client and the data from logs is mapped with
                the billing RDBMS.
17) Tell me the services configured in your cluster(HDFS,YARN,FLUME,HIVE,HUE,SQOOP,OOZIE,Zookeeper).
18) Are you having structure data? Which service is used for it.
                We are having client billing and timesheet related RDBMS data loaded into hadoop using SQOOP, we use sqoop import command 
				to get the data into hive.
				sqoop import --connect jdbc:mysql://localhost:3306/sqoop --username root -P --split-by id --columns id,name 
				--table customer --target-dir /user/cloudera/ingest/raw/customers  --fields-terminated-by "," --hive-import
                --create-hive-table --hive-table sqoop_workspace.customers.
                Here’s what each individual Sqoop command option means:       
                                connect – Provides jdbc string
                                username – Database username
                                -P  – Will ask for the password in console. Alternatively you can use –password but this is not a good practice as its visible in your job execution logs and asking for trouble. One way to deal with this is store database passwords in a file in HDFS and provide at runtime.
                                table – Tells the computer which table you want to import from MySQL. Here, it's customer.
                                split-by – Specifies your splitting column. I am specifying id here.
                                target-dir – HDFS destination directory.
                                fields-terminated-by – I have specified comma (as by default it will import data into HDFS with comma-separated values)
                                hive-import – Import table into Hive (Uses Hive’s default delimiters if none are set.)
                                create-hive-table – Determines if set job will fail if a Hive table already exists. It works in this case.
                                hive-table – Specifies <db_name>.<table_name>. Here it's sqoop_workspace.customers, where sqoop_workspace is my database and customers is the table name.
19) Tell me how sqoop works?
	=> 	Sqoop will connect to the database.
		Sqoop uses JDBC to examine the table by retrieving a list of all the columns and their SQL data types. 
		These SQL types (varchar, integer and more) can then be mapped to Java data types (String, Integer etc.)
		Sqoop’s code generator will use this information to create a table-specific class to hold records extracted from the table.
		Sqoop will connect to cluster and submit a MapReduce job.
		The dataset being transferred is sliced up into different partitions and a map-only job is launched with individual mappers 
		responsible for transferring a slice of this dataset.
		For databases, Sqoop will read the table row-by-row into HDFS. for mainframe datasets, sqoop will read records from each mainframe 
		dataset into HDFS. The output of this import process is a set of files containing a copy of imported table or datasets. 
		The import process is performed in parallel for this reason, the output will be in multiple files. 
		These files may be delimited text files CSV, TSV or binary Avro or Sequence files containing serialized record data. By default it is CSV.
        Sqoop uses MapReduce to import and export the data, which provides parallel operation as well as fault tolerance.
		http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_controlling_parallelism
20) How do you configure Flume?
		Each component (source, sink or channel) in the flow has a name,type,and set of properties that are 
		specific to the type and instantiation.
	Tell me the Flume architecture.
                => Flume Agent consists of Source , Channel , Sink.
21)In HA, what is the use of Quorum Journal Nodes?
22) Do you know about Namenode Fencing?
23) Do you know about Split Brain Condition ?
                => In HA, if active NN goes down, Standby NN becomes Active, and if both NN become active , the fencing method is the solution to avoid the split brain scenario.
24)Linux commands:
                24a) Command to check the port number of a service                    
                                => netstat | grep PORTNUMBER
                24b) Command to check Memory in linux
                                => vmstat , free
25) Have you worked on Hadoop 1. Give me difference between Hadoop v1 and v2?
26) what are the schedulars? (FIFO,Fair,Capacity)
27) Do you know what is ContainerZero?

                =>ContainerZero is first container launched when a job is submitted.
28) Have you generated any reports, (Daily Reports)
                =>Main Menu -> Clusters-> Reports
----------------INFO-------------------------
 
 
-Using snapshot using CM to Back Up data
                -Enable a directory for snapshot.
                -Go to HDFS > File Browser > Particular Directory > Enable Snapshot
                -Snapshot Policy can be created to create a snapshot on regular intervals.
                -To create a Snapshot Policy, go to Main Menu > Backups > Snapshots > Create snapshot Policy.
                -Select particukar service to take snapshot of (HDFS, HBase , Hive)
                -Select daily,weekly,monthly as per requirement to enable the Snapshot Policy.
               
                -To restore files from snapshot, Go to HDFS > File Browser > Directory > Restore Directory from Snapshot>Select snapshot from dropdown list>Restore.
				
				
Q) How to cofigure and troubleshoot mapreduce.
Q) Cloudera Manager Indicators : Red,Green,Amber
-------------------------------------------------------------------------------------
-Hue Configuration in Hortonworks
https://github.com/EsharEditor/ambari-hue-service
-------------------------------------------------------------------------------------

Chapter : 10 : Data Protection,File Formats and Accessing HDFS.
This chapter covers the following:
	-Safeguarding HDFS data using trash and HDFS snapshots
	-Ensuring data integrity with file system checks (fsck command)
	-File-based formats supported by Hadoop
	-Choosing the optimal file format
	-The Hadoop small files problem and merging files
	-Using Hadoop archives to manage small files
	-Using Hadoop WebHDFS and HttpFS
	
You can access HDFS data from outside the Hadoop cluster, through webbased utilities such as WebHDFS and HttpFS, 
and this chapter shows how to use these tools effectively.









Note:
The Text file type is an UNSTRUCTURED file format. Avro, Parquet, RCFile and SequenceFile are all STRUCTURED formats.




	
